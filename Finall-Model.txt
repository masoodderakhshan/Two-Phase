# === Libraries ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns # Added for better visualizations

# --- Ensemble and linear models ---
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# === Plot style ===
plt.rcParams.update({
    'font.family': 'DejaVu Serif',
    'font.size': 14,  # Increased for better readability in articles
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 16,
    'axes.titlepad': 15,
    'figure.dpi': 100 # Default dpi for screen display
})

# === Load data ===
# Ensure the CSV file is accessible. If using Google Colab, upload it first.
try:
    file_path = r"/content/Predicted_Shear_Stress_Scenario.csv"
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print("Error: The data file was not found. Please check the file path.")
    # Creating a dummy dataframe to allow the rest of the script to run
    data = {
        'NS': np.random.rand(100) * 10,
        'W': np.random.rand(100) * 5,
        'RWD': np.random.rand(100) * 20,
        'Stress': np.random.rand(100) * 50,
        'Settlement': np.random.rand(100) * 15,
        'UCS': np.random.rand(100) * 100
    }
    df = pd.DataFrame(data)
    print("Running with a dummy dataset as the original file was not found.")


print("\n=== Data Preview ===")
print(df.head())

# === Select features and target ===
# Corrected the features list to match the dummy data if used
features = ['NS', 'W', 'RWD', 'Stress','Settlement']
target = 'UCS'
X = df[features]
y = df[target]

# === Handle missing values ===
for col in X.columns:
    if X[col].isnull().any():
        X[col].fillna(X[col].median(), inplace=True)
if y.isnull().any():
    y.fillna(y.median(), inplace=True)

# === Standardize features ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === Train/Test split (80/20) ===
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
print(f"\nTrain: {X_train.shape}, Test: {X_test.shape}")

# === Initialize models ===
models = {
    'GradientBoosting': GradientBoostingRegressor(
        n_estimators=80, learning_rate=0.05, max_depth=3, random_state=42),
    'XGBoost': XGBRegressor(
        n_estimators=80, learning_rate=0.05, max_depth=3, random_state=42, objective='reg:squarederror'),
    'RandomForest': RandomForestRegressor(
        n_estimators=100, max_depth=6, random_state=42),
    'Ridge': Ridge(alpha=1.0, random_state=42),
    'Lasso': Lasso(alpha=0.05, random_state=42),
    'ElasticNet': ElasticNet(alpha=0.05, l1_ratio=0.5, random_state=42)
}

# === Train, evaluate, and store results ===
results = {}
trained_models = {} # Dictionary to store trained models

for name, model in models.items():
    print(f"\n=== Training {name} ===")
    model.fit(X_train, y_train)
    trained_models[name] = model # Store the trained model
    y_pred = model.predict(X_test)

    # --- Metrics ---
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    # --- Cross-validation ---
    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()

    print(f"{name} Test R²: {r2:.4f} | CV R² Mean: {cv_mean:.4f}")

    results[name] = {
        'Test_MAE': mae,
        'Test_RMSE': rmse,
        'Test_R2': r2,
        'CV_R2_Mean': cv_mean,
        'CV_R2_Std': cv_std
    }

# === Summary Table ===
results_df = pd.DataFrame(results).T
print("\n=== Model Performance Summary ===")
print(results_df.round(4))

# === Sort by Test R² ===
sorted_results = results_df.sort_values('Test_R2', ascending=False)
print("\n=== Models Sorted by Test R² ===")
print(sorted_results.round(4))


################################################################################
# VISUALIZATIONS FOR ARTICLE
################################################################################

# === 1. Actual vs. Predicted Plot for the Best Model (Article Style) ===
def plot_actual_vs_predicted(y_true, y_pred, mae, rmse, r2, model_name, filename, color):
    """Generates and saves a high-quality actual vs. predicted plot."""
    plt.figure(figsize=(8, 8))
    sns.regplot(x=y_true, y=y_pred, scatter_kws={'alpha':0.6, 'edgecolor':'k', 's': 50, 'color': color},
                line_kws={'color': 'black', 'linestyle': '--', 'linewidth': 1.5}, ci=None)

    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    padding = (max_val - min_val) * 0.05

    # Add Ideal Fit Line (y=x)
    plt.plot(
        [min_val - padding, max_val + padding],
        [min_val - padding, max_val + padding],
        'r--', lw=2, label='Ideal Fit ($y = x$)'
    )

    plt.xlim(min_val - padding, max_val + padding)
    plt.ylim(min_val - padding, max_val + padding)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.xlabel('Actual Stress (kPa)')
    plt.ylabel('Predicted Stress (kPa)')
    plt.title(f'{model_name}: Actual vs. Predicted\n$R^2 = {r2:.4f}$')

    # Add metrics text box
    plt.text(
        0.05, 0.95,
        f'RMSE: {rmse:.4f}\nMAE: {mae:.4f}',
        transform=plt.gca().transAxes,
        fontsize=12, verticalalignment='top',
        bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5)
    )

    plt.legend(loc='lower right')
    plt.grid(True, linestyle='--', color='gray', linewidth=0.5, alpha=0.5)
    plt.tight_layout()
    plt.savefig(filename, dpi=500, bbox_inches='tight')
    plt.show()

# --- Generate plot for the best model ---
best_model_name = sorted_results.index[0]
best_model = trained_models[best_model_name]
y_pred_best = best_model.predict(X_test)
best_metrics = sorted_results.loc[best_model_name]

print(f"\n--- Generating Actual vs. Predicted plot for the best model: {best_model_name} ---")
plot_actual_vs_predicted(
    y_test, y_pred_best,
    best_metrics['Test_MAE'], best_metrics['Test_RMSE'], best_metrics['Test_R2'],
    best_model_name,
    f"{best_model_name}_Actual_vs_Predicted.png",
    color="royalblue"
)

# === 2. XGBoost Feature Importance Plot (Article Style) ===
print("\n--- Generating XGBoost Feature Importance plot ---")
xgb_model = trained_models.get('XGBoost')

if xgb_model:
    importances = xgb_model.feature_importances_
    importance_df = pd.DataFrame({
        'Feature': features,
        'Importance': importances
    }).sort_values('Importance', ascending=False)

    plt.figure(figsize=(10, 6))
    ax = sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis', orient='h')

    # Add data labels
    for p in ax.patches:
        width = p.get_width()
        plt.text(width * 1.01,  # Position x slightly past the bar
                 p.get_y() + p.get_height() / 2, # Position y
                 f'{width:.3f}', # Text
                 va='center')

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.xlabel('Feature Importance (Gini Impurity)')
    plt.ylabel('Feature')
    plt.title('XGBoost Feature Importance', pad=20)
    plt.xlim(0, max(importances) * 1.15) # Adjust x-limit for labels
    plt.tight_layout()
    plt.savefig("XGBoost_Feature_Importance.png", dpi=500, bbox_inches='tight')
    plt.show()
else:
    print("XGBoost model not found in trained models.")


################################################################################
# NEW PLOTTING SECTION: MODEL PERFORMANCE COMPARISON BAR PLOT
################################################################################
print("\n--- Generating Model Performance Comparison Bar Plot ---")

# --- Prepare data for plotting ---
plot_df = sorted_results[['Test_R2', 'CV_R2_Mean', 'Test_RMSE', 'Test_MAE']]
model_names = plot_df.index
metrics_r2 = ['Test_R2', 'CV_R2_Mean']
metrics_error = ['Test_RMSE', 'Test_MAE']

# --- Plotting setup ---
fig, axes = plt.subplots(2, 1, figsize=(12, 14), sharex=True)
colors_r2 = ['#1f77b4', '#aec7e8']    # Professional blue tones for R-squared
colors_error = ['#ff7f0e', '#ffbb78'] # Professional orange tones for Errors
bar_width = 0.35
index = np.arange(len(model_names))

# --- Subplot 1: R-squared Scores ---
ax1 = axes[0]
for i, metric in enumerate(metrics_r2):
    # Position bars side-by-side
    pos = index - bar_width/2 + i * bar_width
    bars = ax1.bar(pos, plot_df[metric], bar_width, label=metric.replace('_', ' '), color=colors_r2[i])
    # Add data labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        ax1.annotate(f'{height:.3f}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 3),  # 3 points vertical offset
                     textcoords="offset points",
                     ha='center', va='bottom', fontsize=10)

ax1.set_ylabel('$R^2$ Score')
ax1.set_title('Model Performance: $R^2$ Scores', pad=15)
ax1.set_ylim(bottom=0) # Start y-axis at 0
ax1.legend(loc='best')
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=.5)

# --- Subplot 2: Error Metrics (MAE and RMSE) ---
ax2 = axes[1]
for i, metric in enumerate(metrics_error):
    # Position bars side-by-side
    pos = index - bar_width/2 + i * bar_width
    bars = ax2.bar(pos, plot_df[metric], bar_width, label=metric.replace('_', ' '), color=colors_error[i])
    # Add data labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        ax2.annotate(f'{height:.3f}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 3),  # 3 points vertical offset
                     textcoords="offset points",
                     ha='center', va='bottom', fontsize=10)

ax2.set_ylabel('Error Value')
ax2.set_title('Model Performance: Error Metrics', pad=15)
ax2.set_ylim(bottom=0) # Start y-axis at 0
ax2.legend(loc='best')
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)
ax2.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=.5)


# --- Final Touches for the entire figure ---
plt.xticks(index, model_names, rotation=25, ha='right')
fig.suptitle('Model Performance Comparison', fontsize=18, y=0.99)
plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust layout to make room for suptitle
plt.savefig("Model_Performance_Bar_Plot.png", dpi=500, bbox_inches='tight')
plt.show()