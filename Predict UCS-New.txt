# === Libraries ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# --- Ensemble and linear models ---
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# === Plot style ===
plt.rcParams.update({
    'font.family': 'DejaVu Serif',
    'font.size': 14,
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 16,
    'axes.titlepad': 15,
    'figure.dpi': 100
})

# === Load data ===
try:
    file_path = r"/content/Predicted_Shear_Stress_Scenario.csv"
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print("Error: The data file was not found. Running with dummy data.")
    data = {
        'NS': np.random.rand(100) * 10,
        'W': np.random.rand(100) * 5,
        'RWD': np.random.rand(100) * 20,
        'Stress': np.random.rand(100) * 50,
        'Settlement': np.random.rand(100) * 15,
        'UCS': np.random.rand(100) * 100
    }
    df = pd.DataFrame(data)

print("\n=== Data Preview ===")
print(df.head())

# === Select features and target ===
features = ['NS', 'W', 'RWD', 'Stress', 'Settlement','Density']
target = 'UCS'
X = df[features]
y = df[target]

# === Handle missing values ===
for col in X.columns:
    if X[col].isnull().any():
        X[col].fillna(X[col].median(), inplace=True)
if y.isnull().any():
    y.fillna(y.median(), inplace=True)

# === Standardize features ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === Train/Test split (80/20) ===
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
print(f"\nTrain: {X_train.shape}, Test: {X_test.shape}")

# === Initialize models ===
models = {
    'GradientBoosting': GradientBoostingRegressor(
        n_estimators=80, learning_rate=0.05, max_depth=3, random_state=42),
    'XGBoost': XGBRegressor(
        n_estimators=80, learning_rate=0.05, max_depth=3, random_state=42, objective='reg:squarederror'),
    'RandomForest': RandomForestRegressor(
        n_estimators=100, max_depth=6, random_state=42),
    'Ridge': Ridge(alpha=1.0, random_state=42),
    'Lasso': Lasso(alpha=0.05, random_state=42),
    'ElasticNet': ElasticNet(alpha=0.05, l1_ratio=0.5, random_state=42)
}

# === Train, evaluate, and store results ===
results = {}
trained_models = {}

for name, model in models.items():
    print(f"\n=== Training {name} ===")
    model.fit(X_train, y_train)
    trained_models[name] = model
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()

    print(f"{name} Test R²: {r2:.4f} | CV R² Mean: {cv_mean:.4f}")

    results[name] = {
        'Test_MAE': mae,
        'Test_RMSE': rmse,
        'Test_R2': r2,
        'CV_R2_Mean': cv_mean,
        'CV_R2_Std': cv_std
    }

# === Summary Table ===
results_df = pd.DataFrame(results).T
print("\n=== Model Performance Summary ===")
print(results_df.round(4))

# === Sort by Test R² ===
sorted_results = results_df.sort_values('Test_R2', ascending=False)
print("\n=== Models Sorted by Test R² ===")
print(sorted_results.round(4))


################################################################################
# VISUALIZATIONS
################################################################################
################################################################################
# TOP 3 MODELS: Actual vs. Predicted in Subplots
################################################################################
print("\n--- Generating Actual vs. Predicted plots for Top 3 Models ---")

# Select top 3 models by Test R²
top3 = sorted_results.head(3)
top3_names = top3.index.tolist()
colors = ['royalblue', 'darkorange', 'seagreen']

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
plt.subplots_adjust(wspace=0.3)
fig.suptitle('Actual vs. Predicted UCS for Top 3 Models', fontsize=18, y=1.03)

for i, (model_name, color) in enumerate(zip(top3_names, colors)):
    model = trained_models[model_name]
    y_pred = model.predict(X_test)
    metrics = top3.loc[model_name]
    ax = axes[i]

    # Scatter + regression line
    sns.regplot(
        x=y_test, y=y_pred, ax=ax,
        scatter_kws={'alpha': 0.6, 'edgecolor': 'k', 's': 50, 'color': color},
        line_kws={'color': 'black', 'linestyle': '--', 'linewidth': 1.5},
        ci=None
    )

    # Ideal fit line (y = x)
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    padding = (max_val - min_val) * 0.05
    ax.plot(
        [min_val - padding, max_val + padding],
        [min_val - padding, max_val + padding],
        'r--', lw=2, label='Ideal Fit ($y = x$)'
    )

    # Format axes
    ax.set_xlim(min_val - padding, max_val + padding)
    ax.set_ylim(min_val - padding, max_val + padding)
    ax.set_aspect('equal', adjustable='box')
    ax.set_xlabel('Actual UCS (kPa)')
    if i == 0:
        ax.set_ylabel('Predicted UCS (kPa)')
    else:
        ax.set_ylabel('')
    ax.set_title(f'{model_name}', fontsize=14)

    # Metrics box
    ax.text(
        0.05, 0.95,
        f'$R^2$ (Test): {metrics["Test_R2"]:.4f}\n'
        f'$R^2$ (CV): {metrics["CV_R2_Mean"]:.4f}\n'
        f'RMSE: {metrics["Test_RMSE"]:.4f}\n'
        f'MAE: {metrics["Test_MAE"]:.4f}',
        transform=ax.transAxes,
        fontsize=11,
        verticalalignment='top',
        bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.6)
    )

    ax.legend(loc='lower right', fontsize=10)
    ax.grid(True, linestyle='--', color='gray', linewidth=0.5, alpha=0.5)

plt.tight_layout()
plt.savefig("Top3_Models_Actual_vs_Predicted.png", dpi=500, bbox_inches='tight')
plt.show()

################################################################################
# XGBoost Feature Importance Plot
################################################################################
print("\n--- Generating XGBoost Feature Importance plot ---")
xgb_model = trained_models.get('XGBoost')

if xgb_model:
    importances = xgb_model.feature_importances_
    importance_df = pd.DataFrame({
        'Feature': features,
        'Importance': importances
    }).sort_values('Importance', ascending=False)

    plt.figure(figsize=(8, 6))
    ax = sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis', orient='h')
    for p in ax.patches:
        width = p.get_width()
        plt.text(width * 1.01, p.get_y() + p.get_height() / 2, f'{width:.3f}', va='center')

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.xlabel('Feature Importance')
    plt.ylabel('Feature')
    plt.title('XGBoost Feature Importance', pad=20)
    plt.xlim(0, max(importances) * 1.15)
    plt.tight_layout()
    plt.savefig("XGBoost_Feature_Importance.png", dpi=500, bbox_inches='tight')
    plt.show()
else:
    print("XGBoost model not found in trained models.")


################################################################################
# MODEL PERFORMANCE COMPARISON BAR PLOT
################################################################################
print("\n--- Generating Model Performance Comparison Bar Plot ---")

plot_df = sorted_results[['Test_R2', 'CV_R2_Mean', 'Test_RMSE', 'Test_MAE']]
model_names = plot_df.index
metrics_r2 = ['Test_R2', 'CV_R2_Mean']
metrics_error = ['Test_RMSE', 'Test_MAE']

fig, axes = plt.subplots(2, 1, figsize=(10, 12), sharex=True)
colors_r2 = ['#1f77b4', '#aec7e8']
colors_error = ['#ff7f0e', '#ffbb78']
bar_width = 0.35
index = np.arange(len(model_names))

# Subplot 1: R²
ax1 = axes[0]
for i, metric in enumerate(metrics_r2):
    pos = index - bar_width / 2 + i * bar_width
    bars = ax1.bar(pos, plot_df[metric], bar_width, label=metric.replace('_', ' '), color=colors_r2[i])
    for bar in bars:
        height = bar.get_height()
        ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                     xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=10)
ax1.set_ylabel('$R^2$ Score')
ax1.set_title('Model Performance: $R^2$ Scores', pad=15)
ax1.legend()
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.yaxis.grid(True, linestyle='--', alpha=.5)

# Subplot 2: Error Metrics
ax2 = axes[1]
for i, metric in enumerate(metrics_error):
    pos = index - bar_width / 2 + i * bar_width
    bars = ax2.bar(pos, plot_df[metric], bar_width, label=metric.replace('_', ' '), color=colors_error[i])
    for bar in bars:
        height = bar.get_height()
        ax2.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                     xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=10)
ax2.set_ylabel('Error Value')
ax2.set_title('Model Performance: Error Metrics', pad=15)
ax2.legend()
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)
ax2.yaxis.grid(True, linestyle='--', alpha=.5)

plt.xticks(index, model_names, rotation=25, ha='right')
fig.suptitle('Model Performance Comparison', fontsize=18, y=0.99)
plt.tight_layout(rect=[0, 0, 1, 0.97])
plt.savefig("Model_Performance_Bar_Plot.png", dpi=500, bbox_inches='tight')
plt.show()
