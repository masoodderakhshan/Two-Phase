import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
from itertools import product
import seaborn as sns

# --- Ensemble methods ---
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from xgboost import XGBRegressor
import lightgbm as lgb

# === Plot style ===
plt.rcParams['font.family'] = 'DejaVu Serif'
plt.rcParams['font.serif'] = ['DejaVu Serif']
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 10
plt.rcParams['figure.titlesize'] = 14
plt.rcParams['axes.titlepad'] = 15

# === File paths (Update these paths if necessary) ===
# Make sure to upload WEKA-3.csv and WEKA-4.csv to your environment
weka3_filepath = r"WEKA-3.csv"
weka4_filepath = r"WEKA-4.csv"

# === Load features ===
print("\n=== Loading input features ===")
df_features = pd.read_csv(weka3_filepath)
print(df_features.head())

# === Load displacement ===
print("\n=== Loading displacement data ===")
df_raw = pd.read_csv(weka4_filepath, header=None)
test_ids = df_raw.iloc[0, 1::2].astype(int).tolist()
disp_steps = df_raw.iloc[2:, 0].astype(float)
kpa_cols = df_raw.iloc[2:, 1::2].astype(float)

print("\n=== Reshaping displacement data ===")
dfs = []
for idx, test_id in enumerate(test_ids):
    df_temp = pd.DataFrame({
        'Displacement_Step': disp_steps,
        'Displacement_kPa': kpa_cols.iloc[:, idx],
        'Test': test_id
    })
    dfs.append(df_temp)
df_displacement = pd.concat(dfs, ignore_index=True)
print(df_displacement.head())

# === Merge ===
df = pd.merge(df_displacement, df_features, on='Test', how='left')
print(df.head())

# === Missing handling ===
print("\n=== Checking missing ===")
print(df.isnull().sum())
for col in df.select_dtypes(include=[np.number]).columns:
    if df[col].isnull().any():
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)
        print(f"Filled missing in {col} with median {median_val}")

# === Features and target ===
features = ['NS', 'Depth', 'W', 'RWD', 'Density', 'Displacement_Step']
X = df[features]
y = df['Displacement_kPa']

# === Scale ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === Train/Val/Test split ===
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"\nTrain: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")

# === Initialize Models ===
models = {
    'GradientBoostingRegressor': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31, random_state=42),
    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)
}

trained_models = {}
results = {}

for name, model in models.items():
    print(f"\n=== Training and evaluating {name} ===")

    model.fit(X_train, y_train)
    trained_models[name] = model

    y_test_pred = model.predict(X_test)
    mae_test = mean_absolute_error(y_test, y_test_pred)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
    r2_test = r2_score(y_test, y_test_pred)

    print(f"\n=== {name} Test Metrics ===")
    print(f"MAE: {mae_test:.4f}")
    print(f"RMSE: {rmse_test:.4f}")
    print(f"R2: {r2_test:.4f}")

    results[name] = {
        'Test_MAE': mae_test, 'Test_RMSE': rmse_test, 'Test_R2': r2_test
    }

    # === Cross-validation ===
    print(f"\n=== {name} Cross-validation ===")
    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
    print(f"Cross-validated R² scores: {cv_scores}")
    print(f"Mean R²: {cv_scores.mean():.4f} | Std: {cv_scores.std():.4f}")

print("\n=== All Models Comparison (Test Set) ===")
results_df = pd.DataFrame(results).T
print(results_df)

# ==============================================================================
# === Predict with LightGBM and Generate Heatmap for Article ===
# ==============================================================================

print("\n=== Generating LightGBM predictions for heatmap ===")

# --- Define the specific parameters for prediction ---
params = {
    'NS': [20, 40, 80],
    'Depth': [10],
    'W': [20, 25, 30, 35],
    'RWD': [0, 1, 2, 3, 4],
    'Density': [1.6],
    'Displacement_Step': [15]
}

# --- Create a DataFrame from all combinations of parameters ---
param_combinations = list(product(*params.values()))
prediction_df = pd.DataFrame(param_combinations, columns=params.keys())

# --- Use the trained scaler to transform the new data ---
# Important: Ensure the order of columns matches the training data
prediction_df_scaled = scaler.transform(prediction_df[features])

# --- Predict 'Displacement_kPa' using the trained LightGBM model ---
lgb_model = trained_models['LightGBM']
prediction_df['Predicted_kPa'] = lgb_model.predict(prediction_df_scaled)

print("\n=== Sample of Predictions ===")
print(prediction_df.head())

# --- Plotting the Heatmaps ---
ns_values = params['NS']
fig, axes = plt.subplots(1, len(ns_values), figsize=(18, 6), sharey=True)

# Determine the global min and max for consistent color scaling across plots
vmin = prediction_df['Predicted_kPa'].min()
vmax = prediction_df['Predicted_kPa'].max()

for i, ns in enumerate(ns_values):
    # Filter data for the current NS value
    subset_df = prediction_df[prediction_df['NS'] == ns]

    # Pivot the data to create the matrix for the heatmap
    pivot_table = subset_df.pivot(index='W', columns='RWD', values='Predicted_kPa')

    # Create the heatmap
    sns.heatmap(pivot_table, ax=axes[i], annot=True, fmt=".2f", cmap="viridis",
                linewidths=.5, vmin=vmin, vmax=vmax,
                cbar_kws={'label': 'Predicted Shear Stress (kPa)'} if i == len(ns_values) - 1 else {})

    axes[i].set_title(f'NS = {ns} kPa')
    axes[i].set_xlabel('Root weight density (RWD)')
    if i == 0:
        axes[i].set_ylabel('Moisture Content (W) %')
    else:
        axes[i].set_ylabel('')


fig.suptitle("Predicted Displacement (kPa) at 15mm Step for Various NS, W, and RWD", fontsize=16, y=1.02)
plt.tight_layout()

# --- Save and show the figure ---
output_fig_path = "LightGBM_Prediction_Heatmap.png"
plt.savefig(output_fig_path, dpi=300, bbox_inches='tight')
print(f"\nHeatmap saved successfully as '{output_fig_path}' (500 DPI).")

plt.show()